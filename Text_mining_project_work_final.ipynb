{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOIQ0cabopVow9ktEkt8OK4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/camilla8989/pythonstudy/blob/TextMining/Text_mining_project_work_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get News headlines"
      ],
      "metadata": {
        "id": "V8BP7LAa43wm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lxml\n",
        "!pip install html5lib\n",
        "\n",
        "import urllib.request\n",
        "import bs4 as bs\n",
        "import yfinance as yf\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def get_news(ticker, start_date, end_date):\n",
        "    current_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
        "    end_date = datetime.strptime(end_date, '%Y-%m-%d')\n",
        "    delta = timedelta(days=1)\n",
        "    headlines = []\n",
        "\n",
        "    while current_date <= end_date:\n",
        "        formatted_date = current_date.strftime('%Y-%m-%d')\n",
        "        \n",
        "        # Fetch news from Google News RSS search\n",
        "        query_date = current_date.strftime('%Y%m%d')\n",
        "        url = f'https://news.google.com/rss/search?q={ticker}+after%3A{query_date}+before%3A{query_date}&hl=en-US&gl=US&ceid=US:en'\n",
        "        try:\n",
        "            doc = urllib.request.urlopen(url).read()\n",
        "            parsed_doc = bs.BeautifulSoup(doc,'lxml')\n",
        "\n",
        "            titles = parsed_doc.find_all('title')\n",
        "            pub_dates = parsed_doc.find_all('pubdate')\n",
        "\n",
        "            daily_headlines = [{'Headline': title.text, 'Date': formatted_date} for title, pub_date in zip(titles, pub_dates)]\n",
        "            headlines.extend(daily_headlines)\n",
        "        except Exception as e:\n",
        "            print(f\"Error on {formatted_date} (Google News): {e}\")\n",
        "\n",
        "        # Fetch news from Yahoo Finance\n",
        "        url = f\"https://finance.yahoo.com/rss/headline?s={ticker}&t={formatted_date}\"\n",
        "        try:\n",
        "            doc = urllib.request.urlopen(url).read()\n",
        "            parsed_doc = bs.BeautifulSoup(doc,'xml')\n",
        "\n",
        "            items = parsed_doc.find_all('item')\n",
        "            daily_headlines = [{'Headline': item.title.text, 'Date': formatted_date} for item in items]\n",
        "            headlines.extend(daily_headlines)\n",
        "        except Exception as e:\n",
        "            print(f\"Error on {formatted_date} (Yahoo Finance): {e}\")\n",
        "        \n",
        "        current_date += delta\n",
        "        time.sleep(15)\n",
        "\n",
        "    return headlines"
      ],
      "metadata": {
        "id": "LD16NEPb76P8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ticker = 'AMAZON'\n",
        "ticker_stock = 'AMZN'\n",
        "start_date = '2022-08-01'\n",
        "end_date = '2023-05-01'\n",
        "\n",
        "headlines = get_news(ticker, start_date, end_date)\n",
        "headlines.to_csv(\"output-1.csv\", index=False)"
      ],
      "metadata": {
        "id": "DDkSCMVp8qsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Stock data and Label "
      ],
      "metadata": {
        "id": "kEzx81uZ43tt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_D9ZeNLZrRGD",
        "outputId": "4256fe66-bad9-4030-ce2a-96b05c720649"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "\n",
        "data = pd.read_csv(\"output-1.csv\")\n",
        "\n",
        "# Use the \"Date\" column as an index and combine rows with the same date into one row\n",
        "data['Date'] = pd.to_datetime(data['Date'])\n",
        "data = data.groupby('Date').agg(lambda x: ', '.join(set(x.dropna()))).reset_index()\n",
        "\n",
        "# Get Amazon stock price from yfinance\n",
        "start_date = data['Date'].min().strftime('%Y-%m-%d')\n",
        "end_date = data['Date'].max().strftime('%Y-%m-%d')\n",
        "apple_stock = yf.download('AMZN', start=start_date, end=end_date)\n",
        "\n",
        "# Calculate daily stock price changes\n",
        "apple_stock['Price_Change'] = apple_stock['Close'].pct_change()\n",
        "\n",
        "# Label stock price changes\n",
        "apple_stock['Label'] = apple_stock['Price_Change'].apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n",
        "\n",
        "# reset index\n",
        "apple_stock = apple_stock.reset_index()\n",
        "\n",
        "# Merge datasets\n",
        "merged_data = pd.merge(data, apple_stock[['Date', 'Label']], on='Date', how='left')\n",
        "\n",
        "# remove rows without stock prices\n",
        "merged_data = merged_data.dropna(subset=['Label'])\n",
        "\n",
        "# convert Label column to integer\n",
        "merged_data['Label'] = merged_data['Label'].astype(int)\n",
        "\n",
        "\n",
        "data = merged_data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SENTIMENT SCORE"
      ],
      "metadata": {
        "id": "Dc6TUPLRCFwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk \n",
        "! pip install nltk\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Read in the data\n",
        "headline = data['Headline']\n",
        "\n",
        "#calculation of sentiment score\n",
        "def calc_sentiment_score(headline):\n",
        "    analyser = SentimentIntensityAnalyzer()\n",
        "    scores = analyser.polarity_scores(headline)\n",
        "    return scores['compound']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnM8_BG6CQgU",
        "outputId": "d98749d7-00d0-4ba6-e190-0545d52e2b76"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_sentiment_score_normalized(headline):\n",
        "    analyser = SentimentIntensityAnalyzer()\n",
        "    scores = analyser.polarity_scores(headline)\n",
        "    \n",
        "    # Define minimum and maximum values for the compound score\n",
        "    min_score = -1\n",
        "    max_score = 1\n",
        "    \n",
        "    # Normalize the compound score to a range of 0 to 1\n",
        "    normalized_score = (scores['compound'] - min_score) / (max_score - min_score)\n",
        "    \n",
        "    return normalized_score\n",
        "\n",
        "analyser = SentimentIntensityAnalyzer()\n",
        "\n",
        "for headline in data['Headline']:\n",
        "    scores = analyser.polarity_scores(headline)\n",
        "    \n",
        "\n",
        "data['sentiment_score'] = data['Headline'].apply(calc_sentiment_score)"
      ],
      "metadata": {
        "id": "nTmYUdRlCU6p"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#normalized sentiment score\n",
        "\n",
        "analyser = SentimentIntensityAnalyzer()\n",
        "\n",
        "for headline in data['Headline']:\n",
        "    scores = analyser.polarity_scores(headline)\n",
        "    \n",
        "\n",
        "data['sentiment_score_normalized'] = data['Headline'].apply(calc_sentiment_score_normalized)\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0uSiTwwDP3h",
        "outputId": "608a6caa-9887-41a5-ecc0-2b0a7da86f92"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          Date                                           Headline  Label  \\\n",
            "0   2022-08-01  NY21: Castelli says he's the moderate Democrat...      0   \n",
            "1   2022-08-02  'Road House': Billy Magnussen, Daniela Melchio...     -1   \n",
            "2   2022-08-03  Roman Kemp Says He 'Contemplated Suicide' In D...      1   \n",
            "3   2022-08-04  \"AMAZON after:20220804 before:20220804\" - Goog...      1   \n",
            "4   2022-08-05  ‘Plucked from thin air’: Paramount plays down ...     -1   \n",
            "..         ...                                                ...    ...   \n",
            "266 2023-04-24  Comcast Tiptoes Into E-Commerce. Here's Why It...     -1   \n",
            "267 2023-04-25  Interview With Value Investor Bill Nygren of O...     -1   \n",
            "268 2023-04-26  Montauk's Lazybones Makes Fishing So Fluking E...      1   \n",
            "269 2023-04-27  Uncle Acid and the Deadbeats' Kevin Starrs on ...      1   \n",
            "270 2023-04-28  ECC's Youngest 2022 Grad Proves Age No Obstacl...     -1   \n",
            "\n",
            "     sentiment_score  sentiment_score_normalized  \n",
            "0             0.9625                     0.98125  \n",
            "1             0.8429                     0.92145  \n",
            "2            -0.3873                     0.30635  \n",
            "3            -0.5695                     0.21525  \n",
            "4            -0.3886                     0.30570  \n",
            "..               ...                         ...  \n",
            "266           0.9547                     0.97735  \n",
            "267           0.8786                     0.93930  \n",
            "268           0.9969                     0.99845  \n",
            "269           0.9963                     0.99815  \n",
            "270           0.9974                     0.99870  \n",
            "\n",
            "[188 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clean data"
      ],
      "metadata": {
        "id": "H4cWVlqH5Ian"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#CLEANING THE DATA \n",
        "import string\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Remove 'Google News' and 'AMAZON after'\n",
        "data['Headline'] = data['Headline'].str.replace(\"Google News\", \"\", regex=False)\n",
        "data['Headline'] = data['Headline'].str.replace(\"AMAZON after\", \"\", regex=False)\n",
        "\n",
        "# Remove punctuation marks\n",
        "merged_data['Headline'] = merged_data['Headline'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
        "\n",
        "# Convert headlines to lowercase\n",
        "merged_data['Headline'] = merged_data['Headline'].str.lower()\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "merged_data['Headline'] = merged_data['Headline'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
        "\n",
        "# Apply stemming\n",
        "stemmer = PorterStemmer()\n",
        "merged_data['Headline'] = merged_data['Headline'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))\n",
        "\n",
        "# Remove non-alphabetic characters\n",
        "merged_data['Headline'] = merged_data['Headline'].apply(lambda x: re.sub(r'[^a-zA-Z]', ' ', x))\n",
        "\n",
        "merged_data.to_csv(\"output_final.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "vNF-1DfGu0-E",
        "outputId": "bda3056d-ac57-416b-e5fd-9e2210954e86"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train data"
      ],
      "metadata": {
        "id": "Kosofdm45YAP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression"
      ],
      "metadata": {
        "id": "7aQyPIu35b4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Read in the data\n",
        "data = pd.read_csv('output_final.csv', encoding = \"ISO-8859-1\")\n",
        "data.head(1)\n",
        "data = data[['Headline','Label']]\n",
        "\n",
        "# Set train set and test set\n",
        "train, test = train_test_split(data, test_size=0.3)\n",
        "headlines = train['Headline'].astype(str).tolist()\n",
        "\n",
        "# Convert to word vector with single word, create sparse matrices \n",
        "basicvectorizer = CountVectorizer(ngram_range=(1,1))\n",
        "basictrain = basicvectorizer.fit_transform(headlines)\n",
        "print(basictrain.shape)\n",
        "\n",
        "# Train Model\n",
        "basicmodel = LogisticRegression()\n",
        "basicmodel = basicmodel.fit(basictrain, train[\"Label\"])\n",
        "\n",
        "# Prediction\n",
        "testheadlines = test['Headline'].astype(str).tolist()\n",
        "basictest = basicvectorizer.transform(testheadlines)\n",
        "predictions = basicmodel.predict(basictest)\n",
        "predictions\n",
        "pd.crosstab(test[\"Label\"], predictions, rownames=[\"Actual\"], colnames=[\"Predicted\"])\n",
        "\n",
        "# Evaluation model\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score \n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "print (classification_report(test[\"Label\"], predictions))\n",
        "print (accuracy_score(test[\"Label\"], predictions))\n",
        "\n",
        "# Convert to word vector with ngram=2 & ngram=3, create sparse matrices \n",
        "basicvectorizer2 = CountVectorizer(ngram_range=(3,3))\n",
        "basictrain2 = basicvectorizer2.fit_transform(headlines)\n",
        "print(basictrain2.shape)\n",
        "\n",
        "basicmodel2 = LogisticRegression()\n",
        "basicmodel2 = basicmodel2.fit(basictrain2, train[\"Label\"])\n",
        "\n",
        "basictest2 = basicvectorizer2.transform(testheadlines)\n",
        "predictions2 = basicmodel2.predict(basictest2)\n",
        "\n",
        "pd.crosstab(test[\"Label\"], predictions2, rownames=[\"Actual\"], colnames=[\"Predicted\"])\n",
        "\n",
        "print (classification_report(test[\"Label\"], predictions2))\n",
        "print (accuracy_score(test[\"Label\"], predictions2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "3K7Lpe_1yaZt",
        "outputId": "69a286a9-a7eb-400a-9b15-51decc79367b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(131, 9237)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.75      0.49      0.59        37\n",
            "           0       0.00      0.00      0.00         1\n",
            "           1       0.39      0.68      0.50        19\n",
            "\n",
            "    accuracy                           0.54        57\n",
            "   macro avg       0.38      0.39      0.36        57\n",
            "weighted avg       0.62      0.54      0.55        57\n",
            "\n",
            "0.543859649122807\n",
            "(131, 38388)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.70      0.81      0.75        37\n",
            "           0       0.00      0.00      0.00         1\n",
            "           1       0.50      0.37      0.42        19\n",
            "\n",
            "    accuracy                           0.65        57\n",
            "   macro avg       0.40      0.39      0.39        57\n",
            "weighted avg       0.62      0.65      0.63        57\n",
            "\n",
            "0.6491228070175439\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT"
      ],
      "metadata": {
        "id": "GpWNubDh5m9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Convert format\n",
        "data = pd.read_csv('output_final.csv', encoding = \"ISO-8859-1\")\n",
        "data['Headline'] = data['Headline'].astype(str)\n",
        "data['Label'] = data['Label'] + 1\n",
        "data['Label'] = data['Label'].astype(int)\n",
        "\n",
        "# Split the data into training and testing sets with a 7:3 ratio\n",
        "train_data, test_data = train_test_split(data, test_size=0.3, random_state=42)\n",
        "\n",
        "!pip install transformers\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.data.iloc[index]\n",
        "        headline = row['Headline']\n",
        "        label = row['Label']\n",
        "        inputs = self.tokenizer(headline, return_tensors=\"pt\", max_length=self.max_len, padding=True, truncation=True)\n",
        "        input_ids = inputs[\"input_ids\"].squeeze()\n",
        "        attention_mask = inputs[\"attention_mask\"].squeeze()\n",
        "\n",
        "        return input_ids, attention_mask, label\n",
        "\n",
        "# Config\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n",
        "model.to(device)\n",
        "\n",
        "max_len = 128\n",
        "batch_size = 16\n",
        "epochs = 5\n",
        "\n",
        "train_dataset = NewsDataset(train_data, tokenizer, max_len)\n",
        "test_dataset = NewsDataset(test_data, tokenizer, max_len)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "uTji2KOUzVXK",
        "outputId": "39284d9a-6b0e-4b02-e5a4-a892518d4f9e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.28.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Model\n",
        "from typing import List\n",
        "import torch\n",
        "\n",
        "def pad_collate_fn(batch: List[torch.Tensor]):\n",
        "    input_ids, attention_mask, labels = zip(*batch)\n",
        "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    attention_mask = torch.nn.utils.rnn.pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    return input_ids, attention_mask, labels\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate_fn)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=pad_collate_fn)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for batch in train_dataloader:\n",
        "        input_ids, attention_mask, labels = [elem.to(device) for elem in batch]\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        loss = loss_fn(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "aNR8kECdzi0P"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation model\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "model.eval()\n",
        "predictions = []\n",
        "ground_truth = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        input_ids, attention_mask, labels = [elem.to(device) for elem in batch]\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        _, preds = torch.max(logits, 1)\n",
        "        predictions.extend(preds.cpu().numpy().tolist())\n",
        "        ground_truth.extend(labels.cpu().numpy().tolist())\n",
        "\n",
        "# Calculate and print classification report\n",
        "report = classification_report(ground_truth, predictions)\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(ground_truth, predictions)\n",
        "print(\"Accuracy Score:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "3Xk5TUGX7CtF",
        "outputId": "01e46884-47ca-4a10-b247-caf7903e5428"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      1.00      0.70        30\n",
            "           2       1.00      0.04      0.07        27\n",
            "\n",
            "    accuracy                           0.54        57\n",
            "   macro avg       0.77      0.52      0.38        57\n",
            "weighted avg       0.76      0.54      0.40        57\n",
            "\n",
            "Accuracy Score: 0.543859649122807\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVM"
      ],
      "metadata": {
        "id": "g4wypcN6_EBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Preprocess the data as needed\n",
        "X = data['Headline'].values\n",
        "y = data['Label'].values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize the data using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Train an SVM classifier with cross-validation\n",
        "clf = svm.SVC(kernel='linear')\n",
        "scores = cross_val_score(clf, X_train_tfidf, y_train, cv=5)\n",
        "\n",
        "# Print the cross-validation scores\n",
        "print('Cross-validation scores:', scores)\n",
        "print('Mean accuracy:', scores.mean())\n",
        "\n",
        "# Train the SVM classifier on the entire training set\n",
        "clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate the performance of the classifier\n",
        "print(confusion_matrix(y_test,y_pred))\n",
        "print(classification_report(y_test,y_pred))\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "47haT9xu_F11",
        "outputId": "3bebea3e-3324-4d50-ddf6-526c1de3e017"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation scores: [0.53333333 0.5        0.53333333 0.46666667 0.5       ]\n",
            "Mean accuracy: 0.5066666666666666\n",
            "[[16  5]\n",
            " [12  5]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.76      0.65        21\n",
            "           2       0.50      0.29      0.37        17\n",
            "\n",
            "    accuracy                           0.55        38\n",
            "   macro avg       0.54      0.53      0.51        38\n",
            "weighted avg       0.54      0.55      0.53        38\n",
            "\n",
            "Accuracy: 0.5526315789473685\n"
          ]
        }
      ]
    }
  ]
}