{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZmsCh+px04JfT+GidTeaf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/camilla8989/pythonstudy/blob/TextMining/TextMining_Assignment_Group_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Get news headlines**\n",
        "1. We take the 25 most popular headlines for each given day in 2017/04/29 to 2023/04/29 from Guardian database by API.\n",
        "\n",
        "  -  How to get API Key\n",
        "  https://bonobo.capi.gutools.co.uk/register/developer\n",
        "2. Output the obtained news headlines to \"output.txt\""
      ],
      "metadata": {
        "id": "ubK0ge3tSSB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import json\n",
        "from pprint import pprint\n",
        "import datetime\n",
        "import csv\n",
        "\n",
        "\n",
        "# apiKey = \"5601dff4-135d-48bb-884e-c8e1c0b15e1f\n",
        "# apiKey = \"79f83d1f-f210-4c8f-81e7-2421f5ea63ed\"\n",
        "apiKey = \"d1e63280-8497-491a-a565-06e979b39727\"\n",
        "\n",
        "date = datetime.datetime(2017, 4, 29,)\n",
        "\n",
        "print(date)\n",
        "\n",
        "\n",
        "for i in range(5000):\n",
        "\n",
        "    date += datetime.timedelta(days=1)\n",
        "    dateBuilt = str(str(date.year) + \"-\" + str(date.month) + \"-\" + str(date.day))\n",
        "    url = str(\"http://content.guardianapis.com/search?from-date=\" + dateBuilt + \"&to-date=\" + dateBuilt + \"&page-size=25&api-key=\" + apiKey)\n",
        "    print(url)\n",
        "    if (dateBuilt == \"2023-4-29\"):\n",
        "        break\n",
        "# url = \"http://content.guardianapis.com/search?from-date=2000-01-01&to-date=2000-01-01&page-size=25&api-key=5601dff4-135d-48bb-884e-c8e1c0b15e1f\"\n",
        "\n",
        "    dateBuiltSlash = str(str(date.year) + \"/\" + str(date.month) + \"/\" + str(date.day))\n",
        "    output = str(dateBuiltSlash)\n",
        "    try:\n",
        "        # Convert from bytes to text\n",
        "        resp_text = urllib.request.urlopen(url).read().decode('UTF-8')\n",
        "    # Use loads to decode from text\n",
        "        json_obj = json.loads(resp_text)\n",
        "\n",
        "\n",
        "        index = 0\n",
        "        while (index < 25):\n",
        "            try:\n",
        "                output = output + \"\\t\" + (json_obj[\"response\"][\"results\"][index][\"webTitle\"])\n",
        "                index = index +1\n",
        "            except:\n",
        "                print(\"ERROR\")\n",
        "                index = index + 1\n",
        "        output = output + \"\\n\"\n",
        "        print(output)\n",
        "\n",
        "        with open(\"output.txt\", \"a\") as myfile:\n",
        "            myfile.write(output)\n",
        "\n",
        "\n",
        "    except:\n",
        "\n",
        "        with open(\"output.txt\", \"a\") as myfile:\n",
        "            myfile.write(dateBuilt)\n"
      ],
      "metadata": {
        "id": "ksKpyXJAR6ox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocessing of news data**\n",
        "1. Due to the limitation of the API, we were unable to obtain all the data, and finally only obtained the news data as of 2022-04-25. Also, there are days where news headlines are empty. So we removed those null values.\n",
        "2. Save the output data of the previous section as a \".csv\" format file. This is more convenient for subsequent data processing."
      ],
      "metadata": {
        "id": "qvWpExrWVFrh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Get the Nasdaq index and label them according to the index change**\n",
        "1. "
      ],
      "metadata": {
        "id": "TW3WnV3yTO2Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "e6vDqqrmqPFR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "\n",
        "# Get the historical data of the Nasdaq index\n",
        "nasdaq = yf.Ticker('^IXIC')\n",
        "data = nasdaq.history(start='2017-04-28', end='2022-04-28')\n",
        "\n",
        "# Compute daily index change\n",
        "daily_returns = data['Close'].pct_change().fillna(0)\n",
        "\n",
        "# Mark daily index change as 1, 0 or -1\n",
        "daily_labels = daily_returns.apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n",
        "\n",
        "# Save daily index changes and labels as a CSV file\n",
        "result = pd.concat([daily_returns, daily_labels], axis=1)\n",
        "result.columns = ['Return', 'Label']\n",
        "result.to_csv('nasdaq_daily_returns.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xlsxwriter\n",
        "import pandas as pd\n",
        "\n",
        "# Read \"nasdaq_daily_returns.xlsx\" file\n",
        "df1 = pd.read_excel('nasdaq_daily_returns.xlsx', sheet_name='Sheet1')\n",
        "\n",
        "# read \"output.xlsx\" file\n",
        "df2 = pd.read_excel('output.xlsx', sheet_name='Sheet1')\n",
        "\n",
        "# Compare the values of the \"Data1\" column with the \"Data\" column, and keep the rows corresponding to the same value\n",
        "merged_df = pd.merge(df1, df2, how='inner', left_on='Date1', right_on='Date')\n",
        "\n",
        "# Save the results to the \"full_data.xlsx\" file\n",
        "merged_df.to_excel('Full_Data.xlsx', sheet_name='Sheet1', index=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rXy9TZvwQiO",
        "outputId": "ab8270ab-7086-49f3-fd10-69eb5b0b9c7c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: xlsxwriter in /usr/local/lib/python3.10/dist-packages (3.1.0)\n"
          ]
        }
      ]
    }
  ]
}